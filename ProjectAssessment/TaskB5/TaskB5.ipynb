{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THO NGUYEN\\AppData\\Local\\Temp\\ipykernel_20348\\2656997453.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\DELL\\Desktop\\COS30018\\COS30018\\ProjectAssessment\\TaskB1\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task B2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Task B3\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Task B4\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN,LSTM, GRU, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B2\n",
    "TICKER = \"AAPL\"\n",
    "START_DATE = \"2010-01-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "LOOK_UP_DAYS = 10      \n",
    "TRAINING_RATIO = 0.8    # 0.7 == 70%\n",
    "SCALE_DATA = True\n",
    "SCALING_METHOD = \"MinMax\"       # MinMax, Standard\n",
    "\n",
    "# Task B3\n",
    "TRADING_PERIOD = 10\n",
    "CONSECUTIVE_DAYS = 300\n",
    "\n",
    "# Task B4\n",
    "NUMBER_OF_LAYER = 1\n",
    "NUMBER_OF_HIDDEN_UNITS = 80\n",
    "MODEL_NAME = \"RNN\"      ## \"RNN\". \"LSTM\", \"GRU\"\n",
    "DROP_OUT_RATE = 0     ## dropout rate in [0,1]\n",
    "NUMBER_OF_EPOCHS = 25\n",
    "BATCH_SIZE = 12\n",
    "FEATURE_PREDICT = \"Close\"    ## \"Open\", \"High\", \"Close\", \"Low\" \n",
    "LOSS_FUNCTION = \"mean_squared_error\" ## \"mean_squared_error\", \"mean_absolute_error\", \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"  ## \"adam\", \"RMSprop\", \"SGD\"\n",
    "\n",
    "# Task B5\n",
    "K_SEQUENCE = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B2: DATA PROCESSING 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataScaler(stock_data, scaling_method=SCALING_METHOD):\n",
    "\n",
    "    DatasetScaler = None\n",
    "    ColumnScalers = {\n",
    "\n",
    "    }\n",
    "    if scaling_method == \"MinMax\":\n",
    "        DatasetScaler = preprocessing.MinMaxScaler()\n",
    "        \n",
    "    \n",
    "    elif scaling_method == \"Standard\":\n",
    "        DatasetScaler = preprocessing.StandardScaler()\n",
    "\n",
    "    \n",
    "    # Learn the whole dataset\n",
    "    col_names = stock_data.columns\n",
    "    features = stock_data[col_names]\n",
    "    DatasetScaler.fit(features.values)\n",
    "    features = DatasetScaler.transform(features.values)\n",
    "    scaledDataFrame = pd.DataFrame(features, columns = col_names)\n",
    "    scaledDataFrame.index = stock_data.index\n",
    "    \n",
    "    for column in col_names:\n",
    "        column_scaler = None\n",
    "        if scaling_method == \"MinMax\":\n",
    "            column_scaler = preprocessing.MinMaxScaler()\n",
    "        elif scaling_method == \"Standard\":\n",
    "            column_scaler = preprocessing.StandardScaler()\n",
    "        column_scaler.fit(stock_data[column].values.reshape(-1,1))\n",
    "        ColumnScalers[column] = column_scaler\n",
    "\n",
    "    return scaledDataFrame, DatasetScaler, ColumnScalers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, scale=SCALE_DATA, scaling_method=SCALING_METHOD):\n",
    "    '''\n",
    "    ticker: is the code of the target ticker\n",
    "    start_date: a start date string with format YYYY/MM/DD\n",
    "    end_date: an end date string with format YYYY/MM/DD\n",
    "    scale: a boolean value, True by default\n",
    "    scaling_method: MinMax(by default), Standard.\n",
    "    '''\n",
    "\n",
    "    # result\n",
    "    result = {\n",
    "        \"dataset\": None,\n",
    "        \"datasetScaler\": None,\n",
    "        \"columnScalers\": None\n",
    "    }\n",
    "\n",
    "    # processing the input parameters\n",
    "    start_date = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "    # creating necessary folder\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    \n",
    "    if not os.path.isdir(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "\n",
    "\n",
    "    \n",
    "    # checking if the data is already downloaded \n",
    "    ## Get a list of files in the directory\n",
    "    files = os.listdir(\"data\")\n",
    "    ## Check each file in the directory\n",
    "    data = None\n",
    "    for file_name in files:\n",
    "        ## if we already downloaded the ticket data\n",
    "        if file_name.startswith(ticker) and file_name.endswith(\".csv\"):\n",
    "            ### Read the file \n",
    "            file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "            data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "            break\n",
    "\n",
    "    ## else, we gonna download the stock data\n",
    "    if data is None:\n",
    "        stock_data = yf.download(ticker, start_date, end_date)\n",
    "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "        stock_data.to_csv(file_path)\n",
    "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "    # if the given time is included in the file, we just take the nessecary dataframe\n",
    "    if data.head(1)[\"Date\"].values[0] <= np.datetime64(start_date) and data.tail(1)[\"Date\"].values[0] >= np.datetime64(end_date):\n",
    "        data = data[(data['Date'] >= pd.to_datetime(start_date)) & (data['Date'] <= pd.to_datetime(end_date))]\n",
    "        print(\"Local Stock Data is enough for requirements, do not need to download\")\n",
    "    else: \n",
    "        stock_data = yf.download(ticker, start_date, end_date)\n",
    "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "        stock_data.to_csv(file_path)\n",
    "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "        print(\"Local Stock Data is not enough for requirements, continuing downloading...\")\n",
    "\n",
    "    # Setting Date as Index\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Scale Data\n",
    "    if scale:\n",
    "        data, scaler, column_scalers = DataScaler(data, scaling_method)\n",
    "        result[\"dataset\"] = data\n",
    "        result[\"datasetScaler\"] = scaler\n",
    "        result[\"columnScalers\"] = column_scalers\n",
    "        return result\n",
    "\n",
    "    result[\"dataset\"] = data\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetSplitter(dataset: pd.DataFrame, look_up_days=LOOK_UP_DAYS, \n",
    "        training_ratio=TRAINING_RATIO,  \n",
    "        feature_columns=['Open','High','Low','Close','Adj Close','Volume'], k_sequence=K_SEQUENCE):\n",
    "    '''\n",
    "    dataset: a Pandas Dataframe\n",
    "    training_ratio: is equal to TRAINING_RATION constant\n",
    "    '''\n",
    "    # result\n",
    "    splitResult = {\n",
    "        \"X_training_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"Y_training_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"X_testing_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"Y_testing_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        }\n",
    "    }\n",
    "  \n",
    "    for column in feature_columns:\n",
    "        dataset_in_column = dataset[column].values.reshape(-1, 1)      # <class 'numpy.ndarray'>\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "    \n",
    "        for x in range(look_up_days, len(dataset_in_column)):\n",
    "            x_data.append(dataset_in_column[x - look_up_days:x, 0])\n",
    "            y_data.append(dataset_in_column[x:x+k_sequence, 0])\n",
    "\n",
    "        splitResult[\"X_training_set\"][column], splitResult[\"X_testing_set\"][column], splitResult[\"Y_training_set\"][column], splitResult['Y_testing_set'][column] = train_test_split(x_data, y_data, test_size=1-training_ratio, shuffle=False)\n",
    "\n",
    "        ## Converting to numpy.array\n",
    "\n",
    "        for column in feature_columns:\n",
    "            splitResult[\"X_training_set\"][column] = np.array(splitResult[\"X_training_set\"][column])\n",
    "            splitResult[\"Y_training_set\"][column] = np.array(splitResult[\"Y_training_set\"][column])\n",
    "            splitResult[\"X_testing_set\"][column] = np.array(splitResult[\"X_testing_set\"][column])\n",
    "            splitResult[\"Y_testing_set\"][column] = np.array(splitResult[\"Y_testing_set\"][column])\n",
    "\n",
    "    return splitResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing 1 (FULL)\n",
    "\n",
    "A function to load and process a dataset with multiple features with the following requirements: \n",
    "\n",
    "+ Specify the start date and the end date for the whole \n",
    "dataset as inputs. \n",
    "+ Allowing you to deal with the NaN issue in the data\n",
    "+ Splitting dataset according to some specified ratio of train/test\n",
    "+ Storing the downloaded data on your local machine for future uses\n",
    "+ Allowing you to have an option to scale your feature columns and store the scalers in a data structure to allow future access to these scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Processing_1():\n",
    "    dataLoader = DataLoader()\n",
    "\n",
    "    scaledStockData = dataLoader[\"dataset\"]\n",
    "    datasetScaler = dataLoader[\"datasetScaler\"]\n",
    "    columnScalers = dataLoader[\"columnScalers\"]\n",
    "    \n",
    "    dataset = datasetSplitter(dataset=scaledStockData)\n",
    "    \n",
    "    print(\"Loaded Done!\\nThe result is a tuple as below:\\n\")\n",
    "    print(\"(dataset, scaledStockData, datasetScaler, columnScalers), where:\\n\")\n",
    "    print(\"dataset is a dictionary as below:\")\n",
    "    print('''{\n",
    "        \"X_training_set\": {\n",
    "                        'Open': <class 'numpy.ndarray'>,\n",
    "                        'High': <class 'numpy.ndarray'>,\n",
    "                        'Low': <class 'numpy.ndarray'>,\n",
    "                        'Close': <class 'numpy.ndarray'>,\n",
    "                        'Adj Close': <class 'numpy.ndarray'>,\n",
    "                        'Volume': <class 'numpy.ndarray'>\n",
    "                        },\n",
    "        \"Y_training_set\": {\n",
    "                        'Open': ...,\n",
    "                        ...\n",
    "                        },\n",
    "        \"X_testing_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        ...\n",
    "                        },\n",
    "        \"Y_testing_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        'Low': ...,\n",
    "                        ...\n",
    "                        }\n",
    "    }\\n''')\n",
    "    print(\"scaledStockData is a Pandas Dataframe of the Stock Ticker (scaled)\\n\")\n",
    "    print(\"datasetScaler is the Scaler of the dataset\\n\")\n",
    "    print(\"columnScalers is a dictionary: each key is a DataFrame Feature('Open', 'High', etc.) and the correspond value is a scaler of that feature\")\n",
    "    return dataset, scaledStockData, datasetScaler, columnScalers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B3: DATA PROCESSING 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetInverser(scaledStockData, datasetScaler):\n",
    "    \n",
    "    # Getting Column name\n",
    "    col_names = scaledStockData.columns\n",
    "    # Inversing the dataframe\n",
    "    re_scaled_features = datasetScaler.inverse_transform(scaledStockData)\n",
    "    re_scaled_stock_data = pd.DataFrame(re_scaled_features, columns = col_names)\n",
    "    # Assigning index to the rescaled_data\n",
    "    re_scaled_stock_data.index = scaledStockData.index\n",
    "\n",
    "\n",
    "    ## A Pandas Dataframe\n",
    "    return re_scaled_stock_data\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candlestick Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandleStick(dataset, datascaler,TradingPeriod=TRADING_PERIOD):\n",
    "    \n",
    "    # Loading the dataset\n",
    "    original_data = None\n",
    "    # rescale the dataset if required\n",
    "    if SCALE_DATA:\n",
    "        original_data = datasetInverser(dataset, datascaler)\n",
    "        \n",
    "    else:\n",
    "        original_data = dataset\n",
    "\n",
    "\n",
    "    # Processed Data to fit the Trading Period\n",
    "    total_records = len(original_data)\n",
    "    Price_Data = {  'Date': [],\n",
    "                    'Open': [],\n",
    "                    'High': [],\n",
    "                    'Low': [],\n",
    "                    'Close': [],\n",
    "                    'Volume': []\n",
    "                }\n",
    "    \n",
    "    \n",
    "    ## Loop through the DataFrame in batches of TradingPeriod\n",
    "    for i in range(0, total_records, TradingPeriod):\n",
    "        batch = original_data.iloc[i:i+TradingPeriod]\n",
    "\n",
    "        Price_Data['Date'].append(batch.index[0])\n",
    "        Price_Data['Open'].append(batch['Open'].values[0])   \n",
    "        Price_Data['High'].append(max(batch['High'].values))\n",
    "        Price_Data['Low'].append(min(batch['Low'].values))\n",
    "        Price_Data['Close'].append(batch['Close'].values[len(batch) - 1])\n",
    "        Price_Data['Volume'].append(sum(batch['Volume'].values))\n",
    "\n",
    "    \n",
    "    # Converting to Pandas Dataframe\n",
    "    NewDataFrame = pd.DataFrame(Price_Data)\n",
    "    NewDataFrame.set_index('Date', inplace=True)\n",
    "\n",
    "    NewDataFrame.info()\n",
    "    fig = go.Figure(data=[go.Candlestick(x=NewDataFrame.index,                         \n",
    "                open=NewDataFrame['Open'],\n",
    "                high=NewDataFrame['High'],\n",
    "                low=NewDataFrame['Low'],\n",
    "                close=NewDataFrame['Close'])])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Candle Stick Chart for {TICKER} shared price, Trading Period = {TRADING_PERIOD} day(s)\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Shared Price (USD)\"\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlot(dataset, datascaler,ConsecutiveDays=CONSECUTIVE_DAYS, features=['Open', 'High', 'Low', 'Close']):\n",
    "    \n",
    "    original_data = None\n",
    "    # rescale the dataset if required\n",
    "    if SCALE_DATA:\n",
    "        original_data = datasetInverser(dataset, datascaler)\n",
    "    else:\n",
    "        original_data = dataset\n",
    "\n",
    "    total_records = len(original_data)\n",
    "\n",
    "    consecutive_days_array = []\n",
    "    ## Loop through the DataFrame in batches of ConsecutiveDays\n",
    "    for i in range(0, total_records, ConsecutiveDays):\n",
    "        batch = original_data.iloc[i:i+ConsecutiveDays]\n",
    "        StartDate = batch.index[0].strftime('%Y-%m-%d')\n",
    "        EndDate = batch.index[len(batch) - 1].strftime('%Y-%m-%d')\n",
    "        \n",
    "        consecutive_period = f\"{StartDate} to {EndDate}\"\n",
    "        for _ in range(len(batch)):\n",
    "            consecutive_days_array.append(consecutive_period)\n",
    "        \n",
    "    original_data['Consecutive'] = consecutive_days_array\n",
    "    original_data['Consecutive'] = original_data['Consecutive'].astype(\"string\")\n",
    "    \n",
    "\n",
    "    for feature in features:\n",
    "        fig = px.box(original_data, x=\"Consecutive\", y=\"Open\")\n",
    "\n",
    "        fig.update_layout(\n",
    "        title=f\"Boxplot Chart for {TICKER} shared price, Consecutive Days = {CONSECUTIVE_DAYS} day(s)\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=f\"{feature} Price (USD)\"\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B3: Data Processing 2 (FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Processing_2():\n",
    "    CandleStick(scaledStockData, datasetScaler)\n",
    "\n",
    "    BoxPlot(scaledStockData, datasetScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B4: Machine Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversing A Column in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnInverser(columnData, ColumnScaler):\n",
    "    '''columnData: an np.array'''\n",
    "    # transform the shape of the data\n",
    "    reshaped_data = columnData.reshape(-1, 1)\n",
    "    result = ColumnScaler.inverse_transform(reshaped_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Loss Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingErrorPloter(loss, validation):\n",
    "    fig = plt.figure(figsize=(6, 4)) \n",
    "    plt.plot(loss, c=\"red\", label = \"training set\")\n",
    "    plt.plot(validation, c=\"blue\", label = \"validation set\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Training Error for {MODEL_NAME} Model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Predicted Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictedResultPloter(actual_value, predicted_value, feature):\n",
    "    fig = plt.figure(figsize=(15, 8)) \n",
    "    plt.plot(actual_value, c=\"black\", label = f\"{TICKER} {feature} Price (Actual)\")\n",
    "    plt.plot(predicted_value, c=\"green\", label = f\"{TICKER} {feature} Price (Predicted)\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{MODEL_NAME} Model,{TICKER} shared price, LOOK_UP_DAYS={LOOK_UP_DAYS}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultSaver(trainedModel,errorFig, predictedFig, modelName=MODEL_NAME):\n",
    "    ## Get a list of foler in the results folders\n",
    "    folders = os.listdir(\"results\")\n",
    "\n",
    "    if modelName not in folders:\n",
    "        # Create the folder for the model\n",
    "        folder_path = os.path.join(\"results\", modelName)\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "    # Access the folder model folder and getting model list\n",
    "    folder_path = os.path.join(\"results\", modelName)\n",
    "    modelLists = os.listdir(folder_path)\n",
    "        \n",
    "    # Creating a new folder to store the training infomation\n",
    "    latestFolderName = \"Model_\" + str(len(modelLists))\n",
    "    model_path = os.path.join(folder_path, latestFolderName)\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "    # Saving Fig\n",
    "    errorFig.savefig(os.path.join(model_path, \"error.png\"))\n",
    "    predictedFig.savefig(os.path.join(model_path, \"predictedResult.png\"))\n",
    "\n",
    "    # Saving model summary\n",
    "    original_stdout = sys.stdout\n",
    "\n",
    "    with open(os.path.join(model_path, \"model_summary.txt\"), 'w') as f:\n",
    "        sys.stdout = f\n",
    "        trainedModel.summary()\n",
    "\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "    ## Writing nessecary infomation to mode_summary.txt\n",
    "    with open(os.path.join(model_path, \"model_summary.txt\"), 'a') as file:\n",
    "        content = f'''\n",
    "        NUMBER_OF_LAYER = {NUMBER_OF_LAYER} \\n\n",
    "        NUMBER_OF_HIDDEN_UNITS = {NUMBER_OF_HIDDEN_UNITS} \\n\n",
    "        NUMBER_OF_EPOCHS = {NUMBER_OF_EPOCHS}\\n\n",
    "        BATCH_SIZE = {BATCH_SIZE}\\n\n",
    "        FEATURE_PREDICT = \"{FEATURE_PREDICT}\"\\n    \n",
    "        LOSS_FUNCTION = \"{LOSS_FUNCTION}\"\\n \n",
    "        OPTIMIZER = \"{OPTIMIZER}\"\\n  \n",
    "        DROP_OUT_RATE = {DROP_OUT_RATE}    '''\n",
    "        \n",
    "        file.write(content)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecurrentNeuralNetworks(layerNums=NUMBER_OF_LAYER,hidden_units=NUMBER_OF_HIDDEN_UNITS, loss_type=LOSS_FUNCTION, optimizerType=OPTIMIZER, dense_unit=1, activation=[\"tanh\", \"linear\"], dropoutRate = DROP_OUT_RATE):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(layerNums):\n",
    "        if i == (layerNums - 1):\n",
    "            model.add(SimpleRNN(hidden_units, activation=activation[0]))\n",
    "            model.add(Dropout(dropoutRate))\n",
    "        else:\n",
    "            model.add(SimpleRNN(hidden_units, activation=activation[0], return_sequences=True))\n",
    "            model.add(Dropout(dropoutRate))\n",
    "\n",
    "    model.add(Dense(units=dense_unit, activation=activation[1]))\n",
    "    model.compile(loss=loss_type, optimizer=optimizerType)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LongShortTermMemory(layerNums=NUMBER_OF_LAYER,hidden_units=NUMBER_OF_HIDDEN_UNITS, loss_type=LOSS_FUNCTION, optimizerType=OPTIMIZER, dense_unit=1, activation=[\"tanh\", \"linear\"], dropoutRate = DROP_OUT_RATE):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(layerNums):\n",
    "        if i == (layerNums - 1):\n",
    "            model.add(LSTM(hidden_units, activation=activation[0]))\n",
    "            model.add(Dropout(dropoutRate))\n",
    "        else:\n",
    "            model.add(LSTM(hidden_units, activation=activation[0], return_sequences=True))\n",
    "            model.add(Dropout(dropoutRate))\n",
    "\n",
    "    model.add(Dense(units=dense_unit, activation=activation[1]))\n",
    "    model.compile(loss=loss_type, optimizer=optimizerType)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GatedRucurrentUnit(layerNums=NUMBER_OF_LAYER,hidden_units=NUMBER_OF_HIDDEN_UNITS, loss_type=LOSS_FUNCTION, optimizerType=OPTIMIZER, dense_unit=1, activation=[\"tanh\", \"linear\"], dropoutRate = DROP_OUT_RATE):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(layerNums):\n",
    "        if i == (layerNums - 1):\n",
    "            model.add(GRU(hidden_units, activation=activation[0]))\n",
    "            model.add(Dropout(dropoutRate))\n",
    "        else:\n",
    "            model.add(GRU(hidden_units, activation=activation[0], return_sequences=True))\n",
    "            model.add(Dropout(dropoutRate))\n",
    "\n",
    "    model.add(Dense(units=dense_unit, activation=activation[1]))\n",
    "    model.compile(loss=loss_type, optimizer=optimizerType)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, dataset, column_scalers, scaled=SCALE_DATA, num_epoc=NUMBER_OF_EPOCHS, batch=BATCH_SIZE, feature=FEATURE_PREDICT):\n",
    "    \n",
    "\n",
    "    ## Getting x_train, y_train, x_test, and y_test\n",
    "    x_train = dataset[\"X_training_set\"][feature]\n",
    "    y_train = dataset[\"Y_training_set\"][feature]\n",
    "    x_test = dataset[\"X_testing_set\"][feature]\n",
    "    y_test = dataset[\"Y_testing_set\"][feature]\n",
    "\n",
    "    ## preprocessing training data\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    ## Training \n",
    "    training_result = model.fit(x_train, y_train, epochs=num_epoc, batch_size=batch, validation_split=0.2)\n",
    "\n",
    "    ## Plotting the error\n",
    "    errorfig = trainingErrorPloter(training_result.history[\"loss\"], training_result.history[\"val_loss\"])\n",
    "    \n",
    "    ## Testing the result\n",
    "    \n",
    "    ### preprocessing the testing data\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "    ### performing prediction\n",
    "    predicted_y_test = model.predict(x_test)\n",
    "    ### Inversing the data if required\n",
    "    if scaled:\n",
    "        y_test = columnInverser(y_test, column_scalers[feature])\n",
    "        predicted_y_test = columnInverser(predicted_y_test, column_scalers[feature])\n",
    "\n",
    "    ## Ploting the Predicted Result\n",
    "    predictedResultFig = predictedResultPloter(y_test, predicted_y_test, feature)\n",
    "\n",
    "    resultSaver(model, errorfig, predictedResultFig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B4: Machine Learning 1 (FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Machine_Learning_1(dataset=dataset, column_scalers=ColumnScalers,modelName=MODEL_NAME):\n",
    "    model = None\n",
    "\n",
    "    if modelName == \"RNN\":\n",
    "        model = RecurrentNeuralNetworks()\n",
    "    elif modelName == \"LSTM\":\n",
    "    #     ## TO DO\n",
    "        model = LongShortTermMemory()\n",
    "    elif modelName == \"GRU\":\n",
    "        model = GatedRucurrentUnit()\n",
    "\n",
    "    ## Performing Training and getting result\n",
    "    Train(model, dataset, column_scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B5: Machine Learning 2\n",
    "\n",
    "Do not run Task B4 because Task B1: Data Processing 1 was modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, scaledStockData, datasetScaler, ColumnScalers  = Data_Processing_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
