{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THO NGUYEN\\AppData\\Local\\Temp\\ipykernel_19928\\1747604047.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Task B2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B2\n",
    "TICKER = \"AMZN\"\n",
    "START_DATE = \"2014-01-01\"\n",
    "END_DATE = \"2020-12-31\"\n",
    "LOOK_UP_DAYS = 30      \n",
    "TRAINING_RATIO = 0.8    # 0.7 == 70%\n",
    "SCALE_DATA = True\n",
    "SCALING_METHOD = \"MinMax\"       # MinMax, Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B2: DATA PROCESSING 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataScaler(stock_data, scaling_method=SCALING_METHOD):\n",
    "\n",
    "    DatasetScaler = None\n",
    "    ColumnScalers = {\n",
    "\n",
    "    }\n",
    "    if scaling_method == \"MinMax\":\n",
    "        DatasetScaler = preprocessing.MinMaxScaler()\n",
    "        \n",
    "    \n",
    "    elif scaling_method == \"Standard\":\n",
    "        DatasetScaler = preprocessing.StandardScaler()\n",
    "\n",
    "    \n",
    "    # Learn the whole dataset\n",
    "    col_names = stock_data.columns\n",
    "    features = stock_data[col_names]\n",
    "    DatasetScaler.fit(features.values)\n",
    "    features = DatasetScaler.transform(features.values)\n",
    "    scaledDataFrame = pd.DataFrame(features, columns = col_names)\n",
    "    scaledDataFrame.index = stock_data.index\n",
    "    \n",
    "\n",
    "    # Learn each column \n",
    "    for column in col_names:\n",
    "        column_scaler = None\n",
    "        if scaling_method == \"MinMax\":\n",
    "            column_scaler = preprocessing.MinMaxScaler()\n",
    "        elif scaling_method == \"Standard\":\n",
    "            column_scaler = preprocessing.StandardScaler()\n",
    "        column_scaler.fit(stock_data[column].values.reshape(-1,1))\n",
    "        ColumnScalers[column] = column_scaler\n",
    "\n",
    "    return scaledDataFrame, DatasetScaler, ColumnScalers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, scale=SCALE_DATA, scaling_method=SCALING_METHOD):\n",
    "    '''\n",
    "    ticker: is the code of the target ticker\n",
    "    start_date: a start date string with format YYYY/MM/DD\n",
    "    end_date: an end date string with format YYYY/MM/DD\n",
    "    scale: a boolean value, True by default\n",
    "    scaling_method: MinMax(by default), Standard.\n",
    "    '''\n",
    "\n",
    "    # result\n",
    "    result = {\n",
    "        \"dataset\": None,\n",
    "        \"datasetScaler\": None,\n",
    "        \"columnScalers\": None\n",
    "    }\n",
    "\n",
    "    # processing the input parameters\n",
    "    start_date = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "    # creating necessary folder\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    \n",
    "    if not os.path.isdir(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "\n",
    "\n",
    "    \n",
    "    # checking if the data is already downloaded \n",
    "    ## Get a list of files in the directory\n",
    "    files = os.listdir(\"data\")\n",
    "    ## Check each file in the directory\n",
    "    data = None\n",
    "    for file_name in files:\n",
    "        ## if we already downloaded the ticket data\n",
    "        if file_name.startswith(ticker) and file_name.endswith(\".csv\"):\n",
    "            ### Read the file \n",
    "            file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "            data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "            break\n",
    "\n",
    "    ## else, we gonna download the stock data\n",
    "    if data is None:\n",
    "        stock_data = yf.download(ticker, start_date, end_date)\n",
    "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "        stock_data.to_csv(file_path)\n",
    "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "    # if the given time is included in the file, we just take the nessecary dataframe\n",
    "    if data.head(1)[\"Date\"].values[0] <= np.datetime64(start_date) and data.tail(1)[\"Date\"].values[0] >= np.datetime64(end_date):\n",
    "        data = data[(data['Date'] >= pd.to_datetime(start_date)) & (data['Date'] <= pd.to_datetime(end_date))]\n",
    "        print(\"Local Stock Data is enough for requirements, do not need to download\")\n",
    "    else: \n",
    "        stock_data = yf.download(ticker, start_date, end_date)\n",
    "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "        stock_data.to_csv(file_path)\n",
    "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "        print(\"Local Stock Data is not enough for requirements, continuing downloading...\")\n",
    "\n",
    "    # Setting Date as Index\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Scale Data\n",
    "    if scale:\n",
    "        data, scaler, column_scalers = DataScaler(data, scaling_method)\n",
    "        result[\"dataset\"] = data\n",
    "        result[\"datasetScaler\"] = scaler\n",
    "        result[\"columnScalers\"] = column_scalers\n",
    "        return result\n",
    "\n",
    "    result[\"dataset\"] = data\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetSplitter(dataset: pd.DataFrame, look_up_days=LOOK_UP_DAYS, \n",
    "        training_ratio=TRAINING_RATIO,  \n",
    "        feature_columns=['Open','High','Low','Close','Adj Close','Volume']):\n",
    "    '''\n",
    "    dataset: a Pandas Dataframe\n",
    "    training_ratio: is equal to TRAINING_RATION constant\n",
    "    '''\n",
    "    # result\n",
    "    splitResult = {\n",
    "        \"X_training_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"Y_training_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"X_testing_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"Y_testing_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        }\n",
    "    }\n",
    "  \n",
    "    for column in feature_columns:\n",
    "        dataset_in_column = dataset[column].values.reshape(-1, 1)      # <class 'numpy.ndarray'>\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "    \n",
    "        for x in range(look_up_days, len(dataset_in_column)):\n",
    "            x_data.append(dataset_in_column[x - look_up_days:x, 0])\n",
    "            y_data.append(dataset_in_column[x, 0])\n",
    "\n",
    "        splitResult[\"X_training_set\"][column], splitResult[\"X_testing_set\"][column], splitResult[\"Y_training_set\"][column], splitResult['Y_testing_set'][column] = train_test_split(x_data, y_data, test_size=1-training_ratio, shuffle=False)\n",
    "\n",
    "        ## Converting to numpy.array\n",
    "\n",
    "        for column in feature_columns:\n",
    "            splitResult[\"X_training_set\"][column] = np.array(splitResult[\"X_training_set\"][column])\n",
    "            splitResult[\"Y_training_set\"][column] = np.array(splitResult[\"Y_training_set\"][column])\n",
    "            splitResult[\"X_testing_set\"][column] = np.array(splitResult[\"X_testing_set\"][column])\n",
    "            splitResult[\"Y_testing_set\"][column] = np.array(splitResult[\"Y_testing_set\"][column])\n",
    "\n",
    "    return splitResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing 1 (FULL)\n",
    "\n",
    "A function to load and process a dataset with multiple features with the following requirements: \n",
    "\n",
    "+ Specify the start date and the end date for the whole \n",
    "dataset as inputs. \n",
    "+ Allowing you to deal with the NaN issue in the data\n",
    "+ Splitting dataset according to some specified ratio of train/test\n",
    "+ Storing the downloaded data on your local machine for future uses\n",
    "+ Allowing you to have an option to scale your feature columns and store the scalers in a data structure to allow future access to these scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Processing_1():\n",
    "    dataLoader = DataLoader()\n",
    "\n",
    "    scaledStockData = dataLoader[\"dataset\"]\n",
    "    datasetScaler = dataLoader[\"datasetScaler\"]\n",
    "    columnScalers = dataLoader[\"columnScalers\"]\n",
    "    \n",
    "    dataset = datasetSplitter(dataset=scaledStockData)\n",
    "    \n",
    "    print(\"Loaded Done!\\nThe result is a tuple as below:\\n\")\n",
    "    print(\"(dataset, scaledStockData, datasetScaler, columnScalers), where:\\n\")\n",
    "    print(\"dataset is a dictionary as below:\")\n",
    "    print('''{\n",
    "        \"X_training_set\": {\n",
    "                        'Open': <class 'numpy.ndarray'>,\n",
    "                        'High': <class 'numpy.ndarray'>,\n",
    "                        'Low': <class 'numpy.ndarray'>,\n",
    "                        'Close': <class 'numpy.ndarray'>,\n",
    "                        'Adj Close': <class 'numpy.ndarray'>,\n",
    "                        'Volume': <class 'numpy.ndarray'>\n",
    "                        },\n",
    "        \"Y_training_set\": {\n",
    "                        'Open': ...,\n",
    "                        ...\n",
    "                        },\n",
    "        \"X_testing_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        ...\n",
    "                        },\n",
    "        \"Y_testing_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        'Low': ...,\n",
    "                        ...\n",
    "                        }\n",
    "    }\\n''')\n",
    "    print(\"scaledStockData is a Pandas Dataframe of the Stock Ticker (scaled)\\n\")\n",
    "    print(\"datasetScaler is the Scaler of the dataset\\n\")\n",
    "    print(\"columnScalers is a dictionary: each key is a DataFrame Feature('Open', 'High', etc.) and the correspond value is a scaler of that feature\")\n",
    "    \n",
    "    return dataset, scaledStockData, datasetScaler, columnScalers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DELL\\Desktop\\COS30018\\COS30018\\ProjectAssessment\\TaskB1\\myenv\\Lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Stock Data is not enough for requirements, continuing downloading...\n",
      "Loaded Done!\n",
      "The result is a tuple as below:\n",
      "\n",
      "(dataset, scaledStockData, datasetScaler, columnScalers), where:\n",
      "\n",
      "dataset is a dictionary as below:\n",
      "{\n",
      "        \"X_training_set\": {\n",
      "                        'Open': <class 'numpy.ndarray'>,\n",
      "                        'High': <class 'numpy.ndarray'>,\n",
      "                        'Low': <class 'numpy.ndarray'>,\n",
      "                        'Close': <class 'numpy.ndarray'>,\n",
      "                        'Adj Close': <class 'numpy.ndarray'>,\n",
      "                        'Volume': <class 'numpy.ndarray'>\n",
      "                        },\n",
      "        \"Y_training_set\": {\n",
      "                        'Open': ...,\n",
      "                        ...\n",
      "                        },\n",
      "        \"X_testing_set\": {\n",
      "                        'Open': ...,\n",
      "                        'High': ...,\n",
      "                        ...\n",
      "                        },\n",
      "        \"Y_testing_set\": {\n",
      "                        'Open': ...,\n",
      "                        'High': ...,\n",
      "                        'Low': ...,\n",
      "                        ...\n",
      "                        }\n",
      "    }\n",
      "\n",
      "scaledStockData is a Pandas Dataframe of the Stock Ticker (scaled)\n",
      "\n",
      "datasetScaler is the Scaler of the dataset\n",
      "\n",
      "columnScalers is a dictionary: each key is a DataFrame Feature('Open', 'High', etc.) and the correspond value is a scaler of that feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset, scaledStockData, datasetScaler, ColumnScalers  = Data_Processing_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.034352</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.054690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>0.034908</td>\n",
       "      <td>0.034425</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.057842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>0.034160</td>\n",
       "      <td>0.032675</td>\n",
       "      <td>0.032604</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.099644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-07</th>\n",
       "      <td>0.033912</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.034236</td>\n",
       "      <td>0.034236</td>\n",
       "      <td>0.045036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-08</th>\n",
       "      <td>0.034963</td>\n",
       "      <td>0.034514</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>0.035435</td>\n",
       "      <td>0.035435</td>\n",
       "      <td>0.062468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-09</th>\n",
       "      <td>0.036569</td>\n",
       "      <td>0.035707</td>\n",
       "      <td>0.035732</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.053176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-10</th>\n",
       "      <td>0.036207</td>\n",
       "      <td>0.034747</td>\n",
       "      <td>0.034284</td>\n",
       "      <td>0.034122</td>\n",
       "      <td>0.034122</td>\n",
       "      <td>0.078268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>0.034813</td>\n",
       "      <td>0.033527</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>0.032063</td>\n",
       "      <td>0.032063</td>\n",
       "      <td>0.085468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-14</th>\n",
       "      <td>0.033020</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.063496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-15</th>\n",
       "      <td>0.035107</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>0.033887</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.078216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close    Volume\n",
       "Date                                                                   \n",
       "2014-01-02  0.035064  0.033398  0.034352  0.034218   0.034218  0.054690\n",
       "2014-01-03  0.034908  0.034425  0.035039  0.033746   0.033746  0.057842\n",
       "2014-01-06  0.034160  0.032675  0.032604  0.032880   0.032880  0.099644\n",
       "2014-01-07  0.033912  0.033126  0.034437  0.034236   0.034236  0.045036\n",
       "2014-01-08  0.034963  0.034514  0.034983  0.035435   0.035435  0.062468\n",
       "2014-01-09  0.036569  0.035707  0.035732  0.035155   0.035155  0.053176\n",
       "2014-01-10  0.036207  0.034747  0.034284  0.034122   0.034122  0.078268\n",
       "2014-01-13  0.034813  0.033527  0.032613  0.032063   0.032063  0.085468\n",
       "2014-01-14  0.033020  0.033175  0.033500  0.034085   0.034085  0.063496\n",
       "2014-01-15  0.035107  0.033383  0.033887  0.033571   0.033571  0.078216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledStockData.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
