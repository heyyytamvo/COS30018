{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THO NGUYEN\\AppData\\Local\\Temp\\ipykernel_19956\\1747604047.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Task B2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B2\n",
    "TICKER = \"AAPL\"\n",
    "START_DATE = \"2014-01-01\"\n",
    "END_DATE = \"2020-12-31\"\n",
    "LOOK_UP_DAYS = 30      \n",
    "TRAINING_RATIO = 0.8    # 0.7 == 70%\n",
    "SPLIT_BY_DATE = False\n",
    "SPLIT_RANDOMLY = False\n",
    "SCALE_DATA = True\n",
    "SCALING_METHOD = \"MinMax\"       # MinMax, Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B2: DATA PROCESSING 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(stock_data, scaling_method=SCALING_METHOD):\n",
    "\n",
    "    if scaling_method == \"MinMax\":\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    elif scaling_method == \"Standard\":\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    \n",
    "        \n",
    "    col_names = stock_data.columns\n",
    "    features = stock_data[col_names]\n",
    "    scaler.fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    scaled_features = pd.DataFrame(features, columns = col_names)\n",
    "    scaled_features.index = stock_data.index\n",
    "    \n",
    "    return scaled_features, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Dataset(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, scale=SCALE_DATA, scaling_method=SCALING_METHOD):\n",
    "    '''\n",
    "    ticker: is the code of the target ticker\n",
    "    start_date: a start date string with format YYYY/MM/DD\n",
    "    end_date: an end date string with format YYYY/MM/DD\n",
    "    scale: a boolean value, True by default\n",
    "    scaling_method: MinMax(by default), Standard.\n",
    "    '''\n",
    "\n",
    "    # result\n",
    "    result = {\n",
    "        \"dataset\": None,\n",
    "        \"scaler\": None\n",
    "    }\n",
    "\n",
    "    # processing the input parameters\n",
    "    start_date = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "    # creating necessary folder\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    \n",
    "    if not os.path.isdir(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "\n",
    "\n",
    "    \n",
    "    # checking if the data is already downloaded \n",
    "    ## Get a list of files in the directory\n",
    "    files = os.listdir(\"data\")\n",
    "    ## Check each file in the directory\n",
    "    data = None\n",
    "    for file_name in files:\n",
    "        ## if we already downloaded the ticket data\n",
    "        if file_name.startswith(ticker) and file_name.endswith(\".csv\"):\n",
    "            ### Read the file \n",
    "            file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "            data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "            break\n",
    "\n",
    "    ## else, we gonna download the stock data\n",
    "    if data is None:\n",
    "        stock_data = yf.download(ticker, start_date, end_date)\n",
    "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "        stock_data.to_csv(file_path)\n",
    "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "    # if the given time is included in the file, we just take the nessecary dataframe\n",
    "    if data.head(1)[\"Date\"].values[0] <= np.datetime64(start_date) and data.tail(1)[\"Date\"].values[0] >= np.datetime64(end_date):\n",
    "        data = data[(data['Date'] >= pd.to_datetime(start_date)) & (data['Date'] <= pd.to_datetime(end_date))]\n",
    "        print(\"Local Stock Data is enough for requirements, do not need to download\")\n",
    "    else: \n",
    "        stock_data = yf.download(ticker, start_date, end_date)\n",
    "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "        stock_data.to_csv(file_path)\n",
    "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "        print(\"Local Stock Data is not enough for requirements, continuing downloading...\")\n",
    "\n",
    "    # Setting Date as Index\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Scale Data\n",
    "    if scale:\n",
    "        data, scaler = scale_data(data, scaling_method)\n",
    "        result[\"dataset\"] = data\n",
    "        result[\"scaler\"] = scaler\n",
    "        return result\n",
    "\n",
    "    result[\"dataset\"] = data\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Stock Data is enough for requirements, do not need to download\n"
     ]
    }
   ],
   "source": [
    "StockData = Load_Dataset(\"AAPL\", \"2010-12-01\", \"2020-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2286 entries, 2010-12-01 to 2019-12-31\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Open       2286 non-null   float64\n",
      " 1   High       2286 non-null   float64\n",
      " 2   Low        2286 non-null   float64\n",
      " 3   Close      2286 non-null   float64\n",
      " 4   Adj Close  2286 non-null   float64\n",
      " 5   Volume     2286 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 125.0 KB\n"
     ]
    }
   ],
   "source": [
    "StockData[\"dataset\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(StockData[\"dataset\"][\"Open\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset: pd.DataFrame, look_up_days=LOOK_UP_DAYS, \n",
    "        training_ratio=TRAINING_RATIO, split_by_date=SPLIT_BY_DATE, \n",
    "        random=SPLIT_RANDOMLY, feature_columns=['Open','High','Low','Close','Adj Close','Volume']):\n",
    "    '''\n",
    "    dataset: a Pandas Dataframe\n",
    "    training_ratio: is equal to TRAINING_RATION constant\n",
    "    split_by_date: is equal to SPLIT_BY_DATE constant\n",
    "    random: is equal to SPLIT_RANDOMLY\n",
    "    '''\n",
    "    # result\n",
    "    result = {\n",
    "        \"X_training_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"Y_training_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"X_testing_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        },\n",
    "        \"Y_testing_set\": {\n",
    "                        'Open': None,\n",
    "                        'High': None,\n",
    "                        'Low': None,\n",
    "                        'Close': None,\n",
    "                        'Adj Close': None,\n",
    "                        'Volume': None\n",
    "                        }\n",
    "    }\n",
    "  \n",
    "    for column in feature_columns:\n",
    "        dataset_in_column = dataset[column].values.reshape(-1, 1)      # <class 'numpy.ndarray'>\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "    \n",
    "        for x in range(look_up_days, len(dataset_in_column)):\n",
    "            x_data.append(dataset_in_column[x - look_up_days:x, 0])\n",
    "            y_data.append(dataset_in_column[x, 0])\n",
    "\n",
    "        result[\"X_training_set\"][column], result[\"X_testing_set\"][column], result[\"Y_training_set\"][column], result['Y_testing_set'][column] = train_test_split(x_data, y_data, test_size=1-training_ratio, shuffle=False)\n",
    "\n",
    "        ## Converting to numpy.array\n",
    "\n",
    "        for column in feature_columns:\n",
    "            result[\"X_training_set\"][column] = np.array(result[\"X_training_set\"][column])\n",
    "            result[\"Y_training_set\"][column] = np.array(result[\"Y_training_set\"][column])\n",
    "            result[\"X_testing_set\"][column] = np.array(result[\"X_testing_set\"][column])\n",
    "            result[\"Y_testing_set\"][column] = np.array(result[\"Y_testing_set\"][column])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing 1 (FULL)\n",
    "\n",
    "A function to load and process a dataset with multiple features with the following requirements: \n",
    "\n",
    "+ Specify the start date and the end date for the whole \n",
    "dataset as inputs. \n",
    "+ Allowing you to deal with the NaN issue in the data\n",
    "+ Splitting dataset according to some specified ratio of train/test\n",
    "+ Storing the downloaded data on your local machine for future uses\n",
    "+ Allowing you to have an option to scale your feature columns and store the scalers in a data structure to allow future access to these scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Processing_1():\n",
    "    StockData = Load_Dataset()\n",
    "\n",
    "    scaledStockData = scale_data(stock_data=StockData[\"dataset\"])\n",
    "\n",
    "    FinalResult = split(dataset=scaledStockData[0])\n",
    "\n",
    "    print(\"Loaded Done!\\nThe result is a dictionary as below:\\n\")\n",
    "    print('''{\n",
    "        \"X_training_set\": {\n",
    "                        'Open': <class 'numpy.ndarray'>,\n",
    "                        'High': <class 'numpy.ndarray'>,\n",
    "                        'Low': <class 'numpy.ndarray'>,\n",
    "                        'Close': <class 'numpy.ndarray'>,\n",
    "                        'Adj Close': <class 'numpy.ndarray'>,\n",
    "                        'Volume': <class 'numpy.ndarray'>\n",
    "                        },\n",
    "        \"Y_training_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        'Low': ...,\n",
    "                        'Close': ...,\n",
    "                        'Adj Close': ...,\n",
    "                        'Volume': ...\n",
    "                        },\n",
    "        \"X_testing_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        'Low': ...,\n",
    "                        'Close': ...,\n",
    "                        'Adj Close': ...,\n",
    "                        'Volume': ...\n",
    "                        },\n",
    "        \"Y_testing_set\": {\n",
    "                        'Open': ...,\n",
    "                        'High': ...,\n",
    "                        'Low': ...,\n",
    "                        'Close': ...,\n",
    "                        'Adj Close': ...,\n",
    "                        'Volume': ...\n",
    "                        }\n",
    "    }''')\n",
    "\n",
    "    return FinalResult\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Stock Data is enough for requirements, do not need to download\n",
      "Loaded Done!\n",
      "The result is a dictionary as below:\n",
      "\n",
      "{\n",
      "        \"X_training_set\": {\n",
      "                        'Open': <class 'numpy.ndarray'>,\n",
      "                        'High': <class 'numpy.ndarray'>,\n",
      "                        'Low': <class 'numpy.ndarray'>,\n",
      "                        'Close': <class 'numpy.ndarray'>,\n",
      "                        'Adj Close': <class 'numpy.ndarray'>,\n",
      "                        'Volume': <class 'numpy.ndarray'>\n",
      "                        },\n",
      "        \"Y_training_set\": {\n",
      "                        'Open': ...,\n",
      "                        'High': ...,\n",
      "                        'Low': ...,\n",
      "                        'Close': ...,\n",
      "                        'Adj Close': ...,\n",
      "                        'Volume': ...\n",
      "                        },\n",
      "        \"X_testing_set\": {\n",
      "                        'Open': ...,\n",
      "                        'High': ...,\n",
      "                        'Low': ...,\n",
      "                        'Close': ...,\n",
      "                        'Adj Close': ...,\n",
      "                        'Volume': ...\n",
      "                        },\n",
      "        \"Y_testing_set\": {\n",
      "                        'Open': ...,\n",
      "                        'High': ...,\n",
      "                        'Low': ...,\n",
      "                        'Close': ...,\n",
      "                        'Adj Close': ...,\n",
      "                        'Volume': ...\n",
      "                        }\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "StockData = Data_Processing_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
