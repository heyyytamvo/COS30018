{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3uoGbrOgy2a"
      },
      "source": [
        "# Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "6Yx92QDKgy2b"
      },
      "outputs": [],
      "source": [
        "# Task B2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Task B3\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Task B4\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Input,SimpleRNN,LSTM, GRU, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "# Task B6\n",
        "from statsmodels.tsa.arima_model import ARIMA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Qm_493gy2c"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "RHaAGHbDgy2c"
      },
      "outputs": [],
      "source": [
        "# Task B2\n",
        "TICKER = \"AAPL\"\n",
        "START_DATE = \"2010-01-01\"\n",
        "END_DATE = \"2023-12-31\"\n",
        "LOOK_UP_DAYS = 2\n",
        "TRAINING_RATIO = 0.8    # 0.7 == 70%\n",
        "SCALE_DATA = True\n",
        "SCALING_METHOD = \"MinMax\"       # MinMax, Standard\n",
        "\n",
        "# Task B3\n",
        "TRADING_PERIOD = 60\n",
        "CONSECUTIVE_DAYS = 300\n",
        "\n",
        "# Task B4\n",
        "NUMBER_OF_LAYER = 2\n",
        "NUMBER_OF_HIDDEN_UNITS = 200\n",
        "MODEL_NAME = \"LSTM\"      ## \"RNN\". \"LSTM\", \"GRU\"\n",
        "DROP_OUT_RATE = 0     ## dropout rate in [0,1]\n",
        "NUMBER_OF_EPOCHS = 10\n",
        "BATCH_SIZE = 12\n",
        "FEATURE_PREDICT = \"Close\"    ## \"Open\", \"High\", \"Close\", \"Low\"\n",
        "LOSS_FUNCTION = \"mean_squared_error\" ## \"mean_squared_error\", \"mean_absolute_error\", \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"  ## \"adam\", \"RMSprop\", \"SGD\"\n",
        "\n",
        "\n",
        "# Task B6\n",
        "START_TEST_DATE = \"2024-01-01\"\n",
        "END_TEST_DATE = \"2024-03-30\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_ROej53gy2c"
      },
      "source": [
        "# Task B2: DATA PROCESSING 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEQ9_IkLgy2c"
      },
      "source": [
        "## Scaling dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "8jwejhtfgy2c"
      },
      "outputs": [],
      "source": [
        "def DataScaler(stock_data, scaling_method=SCALING_METHOD):\n",
        "\n",
        "    DatasetScaler = None\n",
        "    ColumnScalers = {\n",
        "\n",
        "    }\n",
        "    if scaling_method == \"MinMax\":\n",
        "        DatasetScaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "\n",
        "    elif scaling_method == \"Standard\":\n",
        "        DatasetScaler = preprocessing.StandardScaler()\n",
        "\n",
        "\n",
        "    # Learn the whole dataset\n",
        "    col_names = stock_data.columns\n",
        "    features = stock_data[col_names]\n",
        "    DatasetScaler.fit(features.values)\n",
        "    features = DatasetScaler.transform(features.values)\n",
        "    scaledDataFrame = pd.DataFrame(features, columns = col_names)\n",
        "    scaledDataFrame.index = stock_data.index\n",
        "\n",
        "    for column in col_names:\n",
        "        column_scaler = None\n",
        "        if scaling_method == \"MinMax\":\n",
        "            column_scaler = preprocessing.MinMaxScaler()\n",
        "        elif scaling_method == \"Standard\":\n",
        "            column_scaler = preprocessing.StandardScaler()\n",
        "        column_scaler.fit(stock_data[column].values.reshape(-1,1))\n",
        "        ColumnScalers[column] = column_scaler\n",
        "\n",
        "    return scaledDataFrame, DatasetScaler, ColumnScalers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKRKCsrjgy2d"
      },
      "source": [
        "## Loading Data Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "44L8hq-Egy2d"
      },
      "outputs": [],
      "source": [
        "def DataLoader(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, scale=SCALE_DATA, scaling_method=SCALING_METHOD):\n",
        "    '''\n",
        "    ticker: is the code of the target ticker\n",
        "    start_date: a start date string with format YYYY/MM/DD\n",
        "    end_date: an end date string with format YYYY/MM/DD\n",
        "    scale: a boolean value, True by default\n",
        "    scaling_method: MinMax(by default), Standard.\n",
        "    '''\n",
        "\n",
        "    # result\n",
        "    result = {\n",
        "        \"dataset\": None,\n",
        "        \"datasetScaler\": None,\n",
        "        \"columnScalers\": None\n",
        "    }\n",
        "\n",
        "    # processing the input parameters\n",
        "    start_date = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end_date = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "    # creating necessary folder\n",
        "    if not os.path.isdir(\"results\"):\n",
        "        os.mkdir(\"results\")\n",
        "\n",
        "    if not os.path.isdir(\"data\"):\n",
        "        os.mkdir(\"data\")\n",
        "\n",
        "\n",
        "\n",
        "    # checking if the data is already downloaded\n",
        "    ## Get a list of files in the directory\n",
        "    files = os.listdir(\"data\")\n",
        "    ## Check each file in the directory\n",
        "    data = None\n",
        "    for file_name in files:\n",
        "        ## if we already downloaded the ticket data\n",
        "        if file_name.startswith(ticker) and file_name.endswith(\".csv\"):\n",
        "            ### Read the file\n",
        "            file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
        "            data = pd.read_csv(file_path, parse_dates=['Date'])\n",
        "            break\n",
        "\n",
        "    ## else, we gonna download the stock data\n",
        "    if data is None:\n",
        "        stock_data = yf.download(ticker, start_date, end_date)\n",
        "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
        "        stock_data.to_csv(file_path)\n",
        "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
        "\n",
        "    # if the given time is included in the file, we just take the nessecary dataframe\n",
        "    if data.head(1)[\"Date\"].values[0] <= np.datetime64(start_date) and data.tail(1)[\"Date\"].values[0] >= np.datetime64(end_date):\n",
        "        data = data[(data['Date'] >= pd.to_datetime(start_date)) & (data['Date'] <= pd.to_datetime(end_date))]\n",
        "        print(\"Local Stock Data is enough for requirements, do not need to download\")\n",
        "    else:\n",
        "        stock_data = yf.download(ticker, start_date, end_date)\n",
        "        file_path = os.path.join(\"data\", f\"{ticker}.csv\")\n",
        "        stock_data.to_csv(file_path)\n",
        "        data = pd.read_csv(file_path, parse_dates=['Date'])\n",
        "        print(\"Local Stock Data is not enough for requirements, continuing downloading...\")\n",
        "\n",
        "    # Setting Date as Index\n",
        "    data.set_index('Date', inplace=True)\n",
        "\n",
        "    # Scale Data\n",
        "    if scale:\n",
        "        data, scaler, column_scalers = DataScaler(data, scaling_method)\n",
        "        result[\"dataset\"] = data\n",
        "        result[\"datasetScaler\"] = scaler\n",
        "        result[\"columnScalers\"] = column_scalers\n",
        "        return result\n",
        "\n",
        "    result[\"dataset\"] = data\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew9D_W8Ugy2d"
      },
      "source": [
        "## Splitting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "UXE8gjpjgy2d"
      },
      "outputs": [],
      "source": [
        "def datasetSplitter(dataset: pd.DataFrame, look_up_days=LOOK_UP_DAYS, \n",
        "        training_ratio=TRAINING_RATIO,  \n",
        "        feature_columns=['Open','High','Low','Close','Adj Close','Volume']):\n",
        "    '''\n",
        "    dataset: a Pandas Dataframe\n",
        "    training_ratio: is equal to TRAINING_RATION constant\n",
        "    '''\n",
        "    # result\n",
        "    splitResult = {\n",
        "        \"X_training_set\": None,\n",
        "        \"Y_training_set\": None,\n",
        "    }\n",
        "  \n",
        "    x_data = []\n",
        "    y_data = []\n",
        "    close_price_dataframe = dataset[\"Close\"].values.reshape(-1, 1)\n",
        "    for day in range(look_up_days, len(close_price_dataframe)):\n",
        "        x_data.append(close_price_dataframe[day - look_up_days:day, 0])\n",
        "        y_data.append(close_price_dataframe[day, 0])\n",
        "        \n",
        "    splitResult['X_training_set'] = np.array(x_data)\n",
        "    splitResult['Y_training_set'] = np.array(y_data)\n",
        "    \n",
        "    return splitResult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1ZVmvx7gy2d"
      },
      "source": [
        "## Data Processing 1 (FULL)\n",
        "\n",
        "A function to load and process a dataset with multiple features with the following requirements:\n",
        "\n",
        "+ Specify the start date and the end date for the whole\n",
        "dataset as inputs.\n",
        "+ Allowing you to deal with the NaN issue in the data\n",
        "+ Splitting dataset according to some specified ratio of train/test\n",
        "+ Storing the downloaded data on your local machine for future uses\n",
        "+ Allowing you to have an option to scale your feature columns and store the scalers in a data structure to allow future access to these scalers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "5e4_485lgy2d"
      },
      "outputs": [],
      "source": [
        "def Data_Processing_1():\n",
        "    dataLoader = DataLoader()\n",
        "\n",
        "    scaledStockData = dataLoader[\"dataset\"]\n",
        "    datasetScaler = dataLoader[\"datasetScaler\"]\n",
        "    columnScalers = dataLoader[\"columnScalers\"]\n",
        "\n",
        "    dataset = datasetSplitter(dataset=scaledStockData)\n",
        "\n",
        "    print(\"Loaded Done!\\nThe result is a tuple as below:\\n\")\n",
        "    print(\"(dataset, scaledStockData, datasetScaler, columnScalers), where:\\n\")\n",
        "    print(\"dataset is a dictionary as below:\")\n",
        "    print('''{\n",
        "        \"X_training_set\": <class 'numpy.array'>,\n",
        "        \"Y_training_set\": <class 'numpy.array'> \n",
        "    }\\n''')\n",
        "    print(\"scaledStockData is a Pandas Dataframe of the Stock Ticker (scaled)\\n\")\n",
        "    print(\"datasetScaler is the Scaler of the dataset\\n\")\n",
        "    print(\"columnScalers is a dictionary: each key is a DataFrame Feature('Open', 'High', etc.) and the correspond value is a scaler of that feature\")\n",
        "    return dataset, scaledStockData, datasetScaler, columnScalers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cjL2Oh_gy2d",
        "outputId": "fb8b9eca-103e-469d-ca27-8c066c0e9aef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local Stock Data is not enough for requirements, continuing downloading...\n",
            "Loaded Done!\n",
            "The result is a tuple as below:\n",
            "\n",
            "(dataset, scaledStockData, datasetScaler, columnScalers), where:\n",
            "\n",
            "dataset is a dictionary as below:\n",
            "{\n",
            "        \"X_training_set\": {\n",
            "                        'Open': <class 'numpy.ndarray'>,\n",
            "                        'High': <class 'numpy.ndarray'>,\n",
            "                        'Low': <class 'numpy.ndarray'>,\n",
            "                        'Close': <class 'numpy.ndarray'>,\n",
            "                        'Adj Close': <class 'numpy.ndarray'>,\n",
            "                        'Volume': <class 'numpy.ndarray'>\n",
            "                        },\n",
            "        \"Y_training_set\": {\n",
            "                        'Open': ...,\n",
            "                        ...\n",
            "                        },\n",
            "        \"X_testing_set\": {\n",
            "                        'Open': ...,\n",
            "                        'High': ...,\n",
            "                        ...\n",
            "                        },\n",
            "        \"Y_testing_set\": {\n",
            "                        'Open': ...,\n",
            "                        'High': ...,\n",
            "                        'Low': ...,\n",
            "                        ...\n",
            "                        }\n",
            "    }\n",
            "\n",
            "scaledStockData is a Pandas Dataframe of the Stock Ticker (scaled)\n",
            "\n",
            "datasetScaler is the Scaler of the dataset\n",
            "\n",
            "columnScalers is a dictionary: each key is a DataFrame Feature('Open', 'High', etc.) and the correspond value is a scaler of that feature\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset, scaledStockData, datasetScaler, ColumnScalers  = Data_Processing_1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcFVy6twgy2d"
      },
      "source": [
        "# Task B3: DATA PROCESSING 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkpudURTgy2d"
      },
      "source": [
        "## Inverse Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "0rdAinH8gy2e"
      },
      "outputs": [],
      "source": [
        "def datasetInverser(scaledStockData, datasetScaler):\n",
        "\n",
        "    # Getting Column name\n",
        "    col_names = scaledStockData.columns\n",
        "    # Inversing the dataframe\n",
        "    re_scaled_features = datasetScaler.inverse_transform(scaledStockData)\n",
        "    re_scaled_stock_data = pd.DataFrame(re_scaled_features, columns = col_names)\n",
        "    # Assigning index to the rescaled_data\n",
        "    re_scaled_stock_data.index = scaledStockData.index\n",
        "\n",
        "\n",
        "    ## A Pandas Dataframe\n",
        "    return re_scaled_stock_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha-nLG9pgy2e"
      },
      "source": [
        "## Candlestick Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "w3CmALAtgy2e"
      },
      "outputs": [],
      "source": [
        "def CandleStick(dataset, datascaler,TradingPeriod=TRADING_PERIOD):\n",
        "\n",
        "    # Loading the dataset\n",
        "    original_data = None\n",
        "    # rescale the dataset if required\n",
        "    if SCALE_DATA:\n",
        "        original_data = datasetInverser(dataset, datascaler)\n",
        "\n",
        "    else:\n",
        "        original_data = dataset\n",
        "\n",
        "\n",
        "    # Processed Data to fit the Trading Period\n",
        "    total_records = len(original_data)\n",
        "    Price_Data = {  'Date': [],\n",
        "                    'Open': [],\n",
        "                    'High': [],\n",
        "                    'Low': [],\n",
        "                    'Close': [],\n",
        "                    'Volume': []\n",
        "                }\n",
        "\n",
        "\n",
        "    ## Loop through the DataFrame in batches of TradingPeriod\n",
        "    for i in range(0, total_records, TradingPeriod):\n",
        "        batch = original_data.iloc[i:i+TradingPeriod]\n",
        "\n",
        "        Price_Data['Date'].append(batch.index[0])\n",
        "        Price_Data['Open'].append(batch['Open'].values[0])\n",
        "        Price_Data['High'].append(max(batch['High'].values))\n",
        "        Price_Data['Low'].append(min(batch['Low'].values))\n",
        "        Price_Data['Close'].append(batch['Close'].values[len(batch) - 1])\n",
        "        Price_Data['Volume'].append(sum(batch['Volume'].values))\n",
        "\n",
        "\n",
        "    # Converting to Pandas Dataframe\n",
        "    NewDataFrame = pd.DataFrame(Price_Data)\n",
        "    NewDataFrame.set_index('Date', inplace=True)\n",
        "\n",
        "    NewDataFrame.info()\n",
        "    fig = go.Figure(data=[go.Candlestick(x=NewDataFrame.index,\n",
        "                open=NewDataFrame['Open'],\n",
        "                high=NewDataFrame['High'],\n",
        "                low=NewDataFrame['Low'],\n",
        "                close=NewDataFrame['Close'])])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"Candle Stick Chart for {TICKER} shared price, Trading Period = {TRADING_PERIOD} day(s)\",\n",
        "        xaxis_title=\"Time\",\n",
        "        yaxis_title=\"Shared Price (USD)\"\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryeNpVOwgy2e"
      },
      "source": [
        "## Boxplot Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "XN_zZ0kjgy2e"
      },
      "outputs": [],
      "source": [
        "def BoxPlot(dataset, datascaler,ConsecutiveDays=CONSECUTIVE_DAYS, features=['Open', 'High', 'Low', 'Close']):\n",
        "\n",
        "    original_data = None\n",
        "    # rescale the dataset if required\n",
        "    if SCALE_DATA:\n",
        "        original_data = datasetInverser(dataset, datascaler)\n",
        "    else:\n",
        "        original_data = dataset\n",
        "\n",
        "    total_records = len(original_data)\n",
        "\n",
        "    consecutive_days_array = []\n",
        "    ## Loop through the DataFrame in batches of ConsecutiveDays\n",
        "    for i in range(0, total_records, ConsecutiveDays):\n",
        "        batch = original_data.iloc[i:i+ConsecutiveDays]\n",
        "        StartDate = batch.index[0].strftime('%Y-%m-%d')\n",
        "        EndDate = batch.index[len(batch) - 1].strftime('%Y-%m-%d')\n",
        "\n",
        "        consecutive_period = f\"{StartDate} to {EndDate}\"\n",
        "        for _ in range(len(batch)):\n",
        "            consecutive_days_array.append(consecutive_period)\n",
        "\n",
        "    original_data['Consecutive'] = consecutive_days_array\n",
        "    original_data['Consecutive'] = original_data['Consecutive'].astype(\"string\")\n",
        "\n",
        "\n",
        "    for feature in features:\n",
        "        fig = px.box(original_data, x=\"Consecutive\", y=\"Open\")\n",
        "\n",
        "        fig.update_layout(\n",
        "        title=f\"Boxplot Chart for {TICKER} shared price, Consecutive Days = {CONSECUTIVE_DAYS} day(s)\",\n",
        "        xaxis_title=\"Time\",\n",
        "        yaxis_title=f\"{feature} Price (USD)\"\n",
        "        )\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z6vNNFpgy2e"
      },
      "source": [
        "## Task B3: Data Processing 2 (FULL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "Ka6xKLsGgy2e"
      },
      "outputs": [],
      "source": [
        "def Data_Processing_2():\n",
        "    CandleStick(scaledStockData, datasetScaler)\n",
        "\n",
        "    BoxPlot(scaledStockData, datasetScaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JJbPzEEgy2e"
      },
      "source": [
        "# Task B6: Machine Learning 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GtBHqpsgy2e"
      },
      "source": [
        "## Inversing A Column in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "YzkTvVsdgy2e"
      },
      "outputs": [],
      "source": [
        "def columnInverser(columnData, ColumnScaler):\n",
        "    '''columnData: an np.array'''\n",
        "    # transform the shape of the data\n",
        "    reshaped_data = columnData.reshape(-1, 1)\n",
        "    result = ColumnScaler.inverse_transform(reshaped_data)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV7j9EWSgy2e"
      },
      "source": [
        "## Plotting Predicted Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "3T9H1VnXgy2e"
      },
      "outputs": [],
      "source": [
        "def predictedResultPloter(actual_value, predicted_value, feature):\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    plt.plot(actual_value, c=\"black\", label = f\"{TICKER} {feature} Price (Actual)\")\n",
        "    plt.plot(predicted_value, c=\"green\", label = f\"{TICKER} {feature} Price (Predicted)\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"{MODEL_NAME} Model,{TICKER} shared price, LOOK_UP_DAYS={LOOK_UP_DAYS}\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E927YKgngy2e"
      },
      "source": [
        "## Result Saver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "GPW9tgh9gy2e"
      },
      "outputs": [],
      "source": [
        "def resultSaver(trainedModel,errorFig, predictedFig, modelName=MODEL_NAME):\n",
        "    ## Get a list of foler in the results folders\n",
        "    folders = os.listdir(\"results\")\n",
        "\n",
        "    if modelName not in folders:\n",
        "        # Create the folder for the model\n",
        "        folder_path = os.path.join(\"results\", modelName)\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "\n",
        "    # Access the folder model folder and getting model list\n",
        "    folder_path = os.path.join(\"results\", modelName)\n",
        "    modelLists = os.listdir(folder_path)\n",
        "\n",
        "    # Creating a new folder to store the training infomation\n",
        "    latestFolderName = \"Model_\" + str(len(modelLists))\n",
        "    model_path = os.path.join(folder_path, latestFolderName)\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "    # Saving Fig\n",
        "    errorFig.savefig(os.path.join(model_path, \"error.png\"))\n",
        "    predictedFig.savefig(os.path.join(model_path, \"predictedResult.png\"))\n",
        "\n",
        "    # Saving model summary\n",
        "    original_stdout = sys.stdout\n",
        "\n",
        "    with open(os.path.join(model_path, \"model_summary.txt\"), 'w') as f:\n",
        "        sys.stdout = f\n",
        "        trainedModel.summary()\n",
        "\n",
        "    sys.stdout = original_stdout\n",
        "\n",
        "    ## Writing nessecary infomation to mode_summary.txt\n",
        "    with open(os.path.join(model_path, \"model_summary.txt\"), 'a') as file:\n",
        "        content = f'''\n",
        "        NUMBER_OF_LAYER = {NUMBER_OF_LAYER} \\n\n",
        "        NUMBER_OF_HIDDEN_UNITS = {NUMBER_OF_HIDDEN_UNITS} \\n\n",
        "        NUMBER_OF_EPOCHS = {NUMBER_OF_EPOCHS}\\n\n",
        "        BATCH_SIZE = {BATCH_SIZE}\\n\n",
        "        FEATURE_PREDICT = \"{FEATURE_PREDICT}\"\\n\n",
        "        LOSS_FUNCTION = \"{LOSS_FUNCTION}\"\\n\n",
        "        OPTIMIZER = \"{OPTIMIZER}\"\\n\n",
        "        DROP_OUT_RATE = {DROP_OUT_RATE}    '''\n",
        "\n",
        "        file.write(content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWx6Z-TGi-jI",
        "outputId": "218cc0cf-135c-4135-ca8d-3b9f83336961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "(2809, 10)\n"
          ]
        }
      ],
      "source": [
        "print(dataset[\"X_training_set\"][\"Close\"].shape[1])\n",
        "print(dataset[\"X_training_set\"][\"Close\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Only getting the Close Price\n",
        "\n",
        "def testingDataLoader(ticker=TICKER, start_date=START_TEST_DATE, end_date=END_TEST_DATE, scale=SCALE_DATA, scaling_method=SCALING_METHOD):\n",
        "    # result\n",
        "    result = {\n",
        "        \"dataset\": None,\n",
        "        \"datasetScaler\": None,\n",
        "        \"columnScalers\": None\n",
        "    }\n",
        "\n",
        "    # processing the input parameters\n",
        "    start_date = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end_date = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "    \n",
        "    real_stock_data = yf.download(ticker, start_date, end_date)\n",
        "    \n",
        "    # Getting the latest k_days records     \n",
        "    result[\"testingStockData\"] = real_stock_data\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autoregression Intergrated Moving Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = 1   ## last observation\n",
        "d = 1   ## Order of differences\n",
        "q = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arima_mode = ARIMA()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
